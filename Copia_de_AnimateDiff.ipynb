{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/osram90/whisper/blob/main/Copia_de_AnimateDiff.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "### **PROYECTO DILATADO ANIMACIÓN**\n",
        "Basado en:\n",
        "> \"[AnimateDiff](https://animatediff.github.io/): Animate Your Personalized Text-to-Image Diffusion Models without Specific Tuning\"  \n",
        "> by [Yuwei Guo](guoyuwei@pjlab.org.cn), [Ceyuan Yang](https://ceyuan.me/) [Anyi Rao](https://anyirao.com/), [Yaohui Wang](https://wyhsirius.github.io/) [Yu Qiao](https://wyhsirius.github.io/) [Dahua Lin](http://dahua.site/) [Bo Dai](https://daibo.info/)\n",
        "\n",
        "\n",
        "Corre el siguiente paquete para intalar: da clic en play"
      ],
      "metadata": {
        "id": "v04sBovMvKBy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title Instalación de AnimateDiff\n",
        "\n",
        "#################\n",
        "# Install stuff #\n",
        "#################\n",
        "\n",
        "!pip install napm einops omegaconf safetensors diffusers[torch]==0.11.1 transformers\n",
        "!pip install xformers\n",
        "\n",
        "# TODO: use huggingface hub rust downloader, backout aria dependency\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "\n",
        "# Handle uninstallable research dependencies\n",
        "from pathlib import Path\n",
        "import os\n",
        "#os.environ['NAPM_PATH'] = str(Path('.').absolute()) # \"install\" repo into cwd\n",
        "\n",
        "import napm\n",
        "napm.pseudoinstall_git_repo(\"https://github.com/guoyww/animatediff/\", package_name='animatediff', add_install_dir_to_path=True)\n"
      ],
      "metadata": {
        "id": "_E1MsBbbDXXf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5R8KaG34V7h",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Select Models\n",
        "\n",
        "#################\n",
        "# Select models #\n",
        "#################\n",
        "\n",
        "fpath_sd_model = \"/content/models/StableDiffusion/\" # @param {type:\"string\"}\n",
        "fpath_dreambooth_lora = \"/content/models/DreamBooth_LoRA/\" # @param {type:\"string\"}\n",
        "\n",
        "base_model = \"Stable Diffusion v1.5\" # @param [\"Stable Diffusion v1.5\", \"Stable Diffusion v1.4\"]\n",
        "\n",
        "fpath_motion_prior = \"/content/models/Motion_Module/\"\n",
        "\n",
        "\n",
        "\n",
        "################\n",
        "# import stuff #\n",
        "################\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "from pathlib import Path\n",
        "from types import SimpleNamespace\n",
        "\n",
        "import diffusers\n",
        "from diffusers import AutoencoderKL, DDIMScheduler\n",
        "from omegaconf import OmegaConf\n",
        "from safetensors import safe_open\n",
        "import torch\n",
        "from tqdm.auto import tqdm\n",
        "from transformers import CLIPTextModel, CLIPTokenizer\n",
        "\n",
        "# napm needs to be imported before animatediff\n",
        "import napm\n",
        "from animatediff.models.unet import UNet3DConditionModel\n",
        "from animatediff.pipelines.pipeline_animation import AnimationPipeline\n",
        "from animatediff.utils.util import save_videos_grid\n",
        "from animatediff.utils.convert_from_ckpt import convert_ldm_unet_checkpoint, convert_ldm_clip_checkpoint, convert_ldm_vae_checkpoint\n",
        "from animatediff.utils.convert_lora_safetensor_to_diffusers import convert_lora\n",
        "\n",
        "\n",
        "# Being consistent with above in case user doesn't re-run the install cell\n",
        "#os.environ['NAPM_PATH'] = str(Path('.').absolute()) # \"install\" repo into cwd\n",
        "\n",
        "\n",
        "# dig up the config yaml from the napm dependency\n",
        "cfg = napm.config.NapmConfig().load()\n",
        "PKG_ROOT = Path(cfg['packages']['animatediff']['install_dir'])\n",
        "inference_config = OmegaConf.load(PKG_ROOT/\"configs/inference/inference-v1.yaml\")\n",
        "\n",
        "\n",
        "######################\n",
        "# Download as needed #\n",
        "######################\n",
        "\n",
        "\n",
        "if not Path(fpath_sd_model).exists():\n",
        "    !mkdir -p {fpath_sd_model}\n",
        "    !git clone -b fp16 https://huggingface.co/runwayml/stable-diffusion-v1-5 {fpath_sd_model}\n",
        "\n",
        "if not Path(fpath_motion_prior).exists():\n",
        "    !mkdir -p {fpath_motion_prior}\n",
        "    for mm_fname in (\"mm_sd_v14.ckpt\", \"mm_sd_v15.ckpt\"):\n",
        "        !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/{mm_fname} -d {fpath_motion_prior} -o {mm_fname}\n",
        "\n",
        "\n",
        "if not Path(fpath_dreambooth_lora).exists():\n",
        "    !mkdir -p {fpath_dreambooth_lora}\n",
        "    !aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/AnimateDiff/resolve/main/toonyou_beta3.safetensors -d {fpath_dreambooth_lora} -o toonyou_beta3.safetensors\n",
        "\n",
        "############################################\n",
        "\n",
        "mm_fname = \"mm_sd_v15.ckpt\"\n",
        "if base_model != \"Stable Diffusion v1.5\":\n",
        "    mm_fname = \"mm_sd_v14.ckpt\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "\n",
        "# @title Load Models\n",
        "\n",
        "lora_dreambooth_model_path = str(list(Path(fpath_dreambooth_lora).glob('*.safetensors'))[0])\n",
        "motion_module_path = str(Path(fpath_motion_prior)/mm_fname)\n",
        "#unet_additional_kwargs = {}\n",
        "\n",
        "tokenizer    = CLIPTokenizer.from_pretrained(fpath_sd_model, subfolder=\"tokenizer\")\n",
        "text_encoder = CLIPTextModel.from_pretrained(fpath_sd_model, subfolder=\"text_encoder\")\n",
        "vae          = AutoencoderKL.from_pretrained(fpath_sd_model, subfolder=\"vae\")\n",
        "unet         = UNet3DConditionModel.from_pretrained_2d(fpath_sd_model, subfolder=\"unet\", unet_additional_kwargs=OmegaConf.to_container(inference_config.get(\"unet_additional_kwargs\", {})))\n",
        "\n",
        "pipeline = AnimationPipeline(\n",
        "    vae=vae,\n",
        "    text_encoder=text_encoder,\n",
        "    tokenizer=tokenizer,\n",
        "    unet=unet,\n",
        "    scheduler=DDIMScheduler(**OmegaConf.to_container(inference_config.noise_scheduler_kwargs)),\n",
        "    ).to(\"cuda\")\n",
        "\n",
        "\n",
        "# probably wanna change this\n",
        "func_args = SimpleNamespace\n",
        "\n",
        "motion_module_state_dict = torch.load(motion_module_path, map_location=\"cpu\")\n",
        "if \"global_step\" in motion_module_state_dict:\n",
        "  func_args.update({\"global_step\": motion_module_state_dict[\"global_step\"]})\n",
        "missing, unexpected = pipeline.unet.load_state_dict(motion_module_state_dict, strict=False)\n",
        "assert len(unexpected) == 0\n",
        "\n",
        "\n",
        "if lora_dreambooth_model_path != \"\":\n",
        "    if lora_dreambooth_model_path.endswith(\".ckpt\"):\n",
        "        state_dict = torch.load(lora_dreambooth_model_path)\n",
        "        pipeline.unet.load_state_dict(state_dict)\n",
        "\n",
        "    elif lora_dreambooth_model_path.endswith(\".safetensors\"):\n",
        "        state_dict = {}\n",
        "        with safe_open(lora_dreambooth_model_path, framework=\"pt\", device=\"cpu\") as f:\n",
        "            for key in f.keys():\n",
        "                state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        is_lora = all(\"lora\" in k for k in state_dict.keys())\n",
        "        if not is_lora:\n",
        "            base_state_dict = state_dict\n",
        "        else:\n",
        "            base_state_dict = {}\n",
        "            # TODO: deal with model_config.base\n",
        "            with safe_open(model_config.base, framework=\"pt\", device=\"cpu\") as f:\n",
        "                for key in f.keys():\n",
        "                    base_state_dict[key] = f.get_tensor(key)\n",
        "\n",
        "        converted_vae_checkpoint = convert_ldm_vae_checkpoint(base_state_dict, pipeline.vae.config)\n",
        "        pipeline.vae.load_state_dict(converted_vae_checkpoint)\n",
        "\n",
        "        converted_unet_checkpoint = convert_ldm_unet_checkpoint(base_state_dict, pipeline.unet.config)\n",
        "        pipeline.unet.load_state_dict(converted_unet_checkpoint, strict=False)\n",
        "\n",
        "        #pipeline.text_encoder = convert_ldm_clip_checkpoint(base_state_dict)\n",
        "\n",
        "\n",
        "        if is_lora:\n",
        "            pipeline = convert_lora(pipeline, state_dict, alpha=model_config.lora_alpha)\n",
        "\n",
        "pipeline.to(\"cuda\")\n"
      ],
      "metadata": {
        "id": "XMbZhSyvB-9X",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Aqui se genera la animación\n",
        "\n",
        "Redacta el prompt, tiene que ser en inglés :)"
      ],
      "metadata": {
        "id": "Mdr5r6vbaMVG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "\n",
        "n_frames = 24 # @param {type:\"integer\"}\n",
        "width = 448 # @param {type:\"integer\"}\n",
        "height = 320 # @param {type:\"integer\"}\n",
        "\n",
        "# TODO: maybe pl.seed_everything could improve determinism\n",
        "seed = 10788741199826055526 # @param {type:\"integer\"}\n",
        "steps = 30 # @param {type:\"integer\"}\n",
        "guidance_scale = 7.5 # @param {type:\"number\"}\n",
        "prompt = \"an astronaut, floating at the space with the planet earth background\" # @param {type:\"string\"}\n",
        "negative_prompt = \"stationary, motionless, boring, watermark, trademark, copyright, text, shutterstock\" # @param {type:\"string\"}\n",
        "\n",
        "outname = \"sample.gif\" # @param {type:\"string\"}\n",
        "outpath = f\"./{outname}\"\n",
        "\n",
        "sample = pipeline(\n",
        "    prompt=prompt,\n",
        "    negative_prompt     = negative_prompt,\n",
        "    num_inference_steps = steps,\n",
        "    guidance_scale      = guidance_scale,\n",
        "    width               = width,\n",
        "    height              = height,\n",
        "    video_length        = n_frames,\n",
        ").videos\n",
        "\n",
        "samples = torch.concat([sample])\n",
        "\n",
        "save_videos_grid(samples, outpath , n_rows=1)\n",
        "\n",
        "from IPython.display import Image\n",
        "\n",
        "Image(outpath)"
      ],
      "metadata": {
        "id": "g8gLhMqjGszy",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PROYECTO ANIMACIóN"
      ],
      "metadata": {
        "id": "qc3UhVmhanmQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "# @title Instala paquete para convertir de gif a video mp4 y extraer los frames\n",
        "\n",
        "!pip install moviepy ffmpeg-python"
      ],
      "metadata": {
        "cellView": "form",
        "id": "NVabmdBiy1na"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title convierte el gif a video\n",
        "from moviepy.editor import *\n",
        "\n",
        "\n",
        "# Convertir GIF a MP4\n",
        "outname_mp4 = \"sample.mp4\"\n",
        "clip = VideoFileClip(outpath)\n",
        "clip.write_videofile(outname_mp4, codec=\"libx264\", fps=120)\n",
        "\n",
        "# Para visualizar el video en loop en el cuaderno\n",
        "from IPython.display import HTML\n",
        "from base64 import b64encode\n",
        "\n",
        "mp4 = open(outname_mp4,'rb').read()\n",
        "data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n",
        "HTML(\"\"\"\n",
        "<video width=520 controls loop>\n",
        "      <source src=\"%s\" type=\"video/mp4\">\n",
        "</video>\n",
        "\"\"\" % data_url)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "6Wvk5VtLyhDU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Extracción de Frames del video\n",
        "## @subtitle Basado en el siguiente cuaderno: VideoFrameandAudioextraction [ 'https://colab.research.google.com/drive/1OJIVza67wA4EiFUaJPX2lhEU7y_9iayM#scrollTo=nbVXuLyNwRot&line=2&uniqifier=1']\n",
        "#################\n",
        "# Editor de video #\n",
        "#################\n",
        "from moviepy.editor import *\n",
        "import ffmpeg\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "# Convertir GIF a MP4\n",
        "outname = \"sample.gif\"\n",
        "outpath = f\"./{outname}\"\n",
        "outname_mp4 = \"sample.mp4\"\n",
        "clip = VideoFileClip(outpath)\n",
        "clip.write_videofile(outname_mp4, codec=\"libx264\", fps=24)  # Asegúrate de que el FPS sea correcto para tu video\n",
        "\n",
        "# Función modificada para extraer todos los frames del video MP4\n",
        "def extract_frames_from_video(video_path, output_dir):\n",
        "    if not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    ffmpeg.input(video_path).output(f'{output_dir}/frame%04d.jpg', qscale=0).run()\n",
        "\n",
        "# Directorio donde se guardarán los frames extraídos\n",
        "frames_output_dir = \"./frames\"\n",
        "# Extraer todos los frames del video MP4\n",
        "extract_frames_from_video(outname_mp4, frames_output_dir)\n",
        "\n",
        "# Mostrar información sobre los frames extraídos\n",
        "print(f\"Frames extraídos en {frames_output_dir}\")\n",
        "\n",
        "import os\n",
        "from IPython.display import Image, display\n",
        "\n",
        "# Ruta al directorio donde se encuentran los frames\n",
        "frames_directory = './frames'\n",
        "frames = [os.path.join(frames_directory, f) for f in os.listdir(frames_directory) if f.endswith('.jpg')]\n",
        "\n",
        "# Ordena los frames por nombre, si es necesario\n",
        "frames.sort()\n",
        "\n",
        "# Visualiza los frames\n",
        "for frame_path in frames:\n",
        "    display(Image(filename=frame_path))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "nbVXuLyNwRot"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m-2JxeN80YOp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}